# update_db.py
import os
import json
import uuid
from tqdm import tqdm
from qdrant_client import QdrantClient, models
from langchain_community.document_loaders import JSONLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
import torch

# --- 1. ì„¤ì • ì˜ì—­ ---
# DigitalOcean DB ì„œë²„ì˜ IP ì£¼ì†Œë¥¼ ì…ë ¥í•˜ì„¸ìš”.
QDRANT_HOST = "159.223.73.50"

# Qdrantì— ìƒì„±ëœ ì»¬ë ‰ì…˜(ì €ì¥ì†Œ) ì´ë¦„
COLLECTION_NAME = "qdrant-franchise-db"

# ìƒˆë¡œ ì¶”ê°€í•  ë°ì´í„°ê°€ ë‹´ê¸´ ì›ë³¸ íŒŒì¼ ê²½ë¡œ
NEW_DATA_PATH = "./new_franchise_data.jsonl"

# ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸
EMBEDDING_MODEL_NAME = "nlpai-lab/KURE-v1"
# -----------------------------


def load_and_split_documents(file_path):
    """ìƒˆë¡œìš´ JSONL ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì‘ì€ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
    print(f"'{file_path}'ì—ì„œ ìƒˆë¡œìš´ ë¬¸ì„œë¥¼ ë¡œë”©í•©ë‹ˆë‹¤...")
    loader = JSONLoader(
        file_path=file_path,
        jq_schema='.',
        content_key="page_content",
        json_lines=True,
        metadata_func=lambda record, metadata: record.get("metadata", {})
    )
    documents = loader.load()
    
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    split_docs = text_splitter.split_documents(documents)
    print(f"ìƒˆë¡œìš´ ë¬¸ì„œë¥¼ {len(split_docs)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")
    return split_docs


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # 1. ìƒˆë¡œìš´ ë¬¸ì„œ ë¡œë”© ë° ë¶„í• 
    document_chunks = load_and_split_documents(NEW_DATA_PATH)

    # 2. ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    print(f"ì„ë² ë”© ëª¨ë¸ '{EMBEDDING_MODEL_NAME}'ì„ ë¡œë”©í•©ë‹ˆë‹¤...")
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    embedding_model = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL_NAME,
        model_kwargs={'device': device},
        encode_kwargs={'normalize_embeddings': True}
    )

    # 3. DigitalOcean DB ì„œë²„ì— ì—°ê²°
    print(f"DigitalOcean DB ì„œë²„({QDRANT_HOST})ì— ì—°ê²°í•©ë‹ˆë‹¤...")
    client = QdrantClient(host=QDRANT_HOST, port=6333, timeout=60)

    # 4. ìƒˆë¡œìš´ ë¬¸ì„œ ì²­í¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
    print("ìƒˆë¡œìš´ ë¬¸ì„œ ì²­í¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...")
    chunk_contents = [doc.page_content for doc in document_chunks]
    vectors = embedding_model.embed_documents(chunk_contents)
    
    # 5. Qdrantì— ì—…ë¡œë“œí•  ë°ì´í„° ì¤€ë¹„
    points_to_upload = []
    for i, doc in enumerate(document_chunks):
        payload = {"page_content": doc.page_content, **doc.metadata}
        points_to_upload.append(
            models.PointStruct(id=str(uuid.uuid4()), vector=vectors[i], payload=payload)
        )

    # 6. DigitalOcean DB ì„œë²„ì— ë°ì´í„° ì¶”ê°€ (Upsert)
    print(f"{len(points_to_upload)}ê°œì˜ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ DB ì„œë²„ì— ì¶”ê°€í•©ë‹ˆë‹¤...")
    client.upsert(
        collection_name=COLLECTION_NAME,
        points=points_to_upload,
        wait=True,
        batch_size=128
    )

    print("\nğŸ‰ ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    collection_info = client.get_collection(collection_name=COLLECTION_NAME)
    print(f"í˜„ì¬ ì»¬ë ‰ì…˜ì˜ ì´ í¬ì¸íŠ¸ ìˆ˜: {collection_info.points_count}")


if __name__ == "__main__":
    main()
