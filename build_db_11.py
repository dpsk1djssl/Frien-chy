# build_production_db_v2.py
import os
import json
import time
import gc
from glob import glob
from typing import List, Iterator

from tqdm import tqdm
import torch
from langchain.schema import Document
from langchain_qdrant import Qdrant
from langchain_huggingface import HuggingFaceEmbeddings
from qdrant_client import QdrantClient, models
import ijson # ëŒ€ìš©ëŸ‰ JSON ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from huggingface_hub import snapshot_download # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•´ ì¶”ê°€

# ==============================================================================
# âš™ï¸ 1. ì‚¬ìš©ì ì„¤ì • ì˜ì—­ (ì´ ë¶€ë¶„ë§Œ ìˆ˜ì •í•´ì£¼ì„¸ìš”!)
# ==============================================================================

# 1-1. ë°ì´í„° ê²½ë¡œ ì„¤ì •
DATA_ROOT = "./finaldata"

# 1-2. Qdrant ì„œë²„ ì„¤ì •
QDRANT_URL = "http://165.22.105.79:6333"
API_KEY = None
COLLECTION_NAME = "qdrant-franchise-db"

# 1-3. ì„ë² ë”© ëª¨ë¸ ë° ì„±ëŠ¥ ì„¤ì •
MODEL_NAME = "nlpai-lab/KURE-v1"
EMBEDDING_BATCH_SIZE = 64
DOCUMENT_UPLOAD_BATCH_SIZE = 64

# ==============================================================================
# ğŸ› ï¸ 2. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (ìˆ˜ì •í•  í•„ìš” ì—†ìŒ)
# ==============================================================================

def download_model_with_progress(model_name: str):
    """
    Hugging Face Hubì—ì„œ ëª¨ë¸ì„ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œí•˜ê³  ì§„í–‰ ìƒí™©ì„ í‘œì‹œí•©ë‹ˆë‹¤.
    """
    print(f"'{model_name}' ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. (ì§„í–‰ë¥  í‘œì‹œ)")
    try:
        snapshot_download(
            repo_id=model_name,
            local_dir_use_symlinks=False, # ìœˆë„ìš° í˜¸í™˜ì„±
            resume_download=True # ì´ì–´ë°›ê¸° ê¸°ëŠ¥ í™œì„±í™”
        )
        print("âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ.")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ê±°ë‚˜, ëª¨ë¸ ì´ë¦„ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.")
        raise

def create_comprehensive_document(data: dict) -> str:
    # (ê¸°ì¡´ê³¼ ë™ì¼í•œ í•¨ìˆ˜)
    ql = data.get('QL', {}) or {}
    jng = data.get('JNG_INFO', {}) or {}
    attr = data.get('ATTRB_INFO', {}) or {}
    brand = jng.get('BRAND_NM', '') or ''
    hq = jng.get('JNGHDQRTRS_CONM_NM', '') or ''
    parts = [f"[ë¸Œëœë“œ] {brand}", f"[ê°€ë§¹ë³¸ë¶€] {hq}", ""]
    orig = (ql.get('ORIGINAL_TEXT') or '')
    if orig: parts.extend(['[ì›ë³¸]', orig, ""])
    abstracted = (ql.get('ABSTRACTED_SUMMARY_TEXT') or '').strip()
    if abstracted: parts.extend(['[ì¶”ìƒ ìš”ì•½]', abstracted, ""])
    extracted = (ql.get('EXTRACTED_SUMMARY_TEXT') or '').strip()
    if extracted: parts.extend(['[ì¶”ì¶œ ìš”ì•½]', extracted, ""])
    return '\n'.join(parts)

def create_enhanced_metadata(data: dict, file_name: str, idx: int) -> dict:
    # (ê¸°ì¡´ê³¼ ë™ì¼í•œ í•¨ìˆ˜)
    jng = data.get('JNG_INFO', {}) or {}
    attr = data.get('ATTRB_INFO', {}) or {}
    return {
        'brand_name': jng.get('BRAND_NM', '') or '',
        'headquarters_name': jng.get('JNGHDQRTRS_CONM_NM', '') or '',
        'source_file': file_name,
        'index_in_file': idx,
    }

def stream_documents_from_json(file_path: str) -> Iterator[Document]:
    """
    ëŒ€ìš©ëŸ‰ JSON íŒŒì¼ì„ í†µì§¸ë¡œ ì½ì§€ ì•Šê³ , ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ í•˜ë‚˜ì”© ì½ì–´
    Document ê°ì²´ë¥¼ ìƒì„±í•˜ëŠ” ì œë„ˆë ˆì´í„°(Generator) í•¨ìˆ˜ì…ë‹ˆë‹¤.
    """
    with open(file_path, "r", encoding="utf-8") as f:
        # ijson.itemsëŠ” íŒŒì¼ ì „ì²´ë¥¼ ë©”ëª¨ë¦¬ì— ì˜¬ë¦¬ì§€ ì•Šê³ , 'item' í‚¤ ì•„ë˜ì˜ ê°ì²´ë“¤ì„ í•˜ë‚˜ì”© ê°€ì ¸ì˜µë‹ˆë‹¤.
        records = ijson.items(f, 'item')
        for i, data in enumerate(records):
            try:
                content = create_comprehensive_document(data)
                if content.strip():
                    metadata = create_enhanced_metadata(data, os.path.basename(file_path), i)
                    yield Document(page_content=content, metadata=metadata)
            except Exception as e:
                print(f"  âš ï¸ í•­ëª© {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue

def aggressive_memory_cleanup():
    """GPU ë©”ëª¨ë¦¬ë¥¼ ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    gc.collect()

# ==============================================================================
# ğŸš€ 3. ë©”ì¸ ì‹¤í–‰ ë¡œì§
# ==============================================================================
def main():
    start_time = time.time()
    
    # 0. ëª¨ë¸ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œ (ì§„í–‰ë¥  í‘œì‹œ)
    download_model_with_progress(MODEL_NAME)

    # 1. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
    print(f"ğŸ¤– ì„ë² ë”© ëª¨ë¸ '{MODEL_NAME}'ì„ ë¡œë”©í•©ë‹ˆë‹¤...")
    embedding_model = HuggingFaceEmbeddings(
        model_name=MODEL_NAME,
        model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},
        encode_kwargs={'batch_size': EMBEDDING_BATCH_SIZE, 'normalize_embeddings': True}
    )

    # 2. Qdrant ì„œë²„ ì—°ê²° ë° DB ì´ˆê¸°í™”
    print(f"\nğŸ”Œ Qdrant ì„œë²„({QDRANT_URL})ì— ì—°ê²°í•©ë‹ˆë‹¤...")
    client = QdrantClient(url=QDRANT_URL, api_key=API_KEY, timeout=60)
    print(f"ğŸ”¥ ê¸°ì¡´ '{COLLECTION_NAME}' ì»¬ë ‰ì…˜ì„ ì‚­ì œí•˜ê³  ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤...")
    client.recreate_collection(
        collection_name=COLLECTION_NAME,
        vectors_config=models.VectorParams(size=1024, distance=models.Distance.COSINE),
    )
    vectorstore = Qdrant(client=client, collection_name=COLLECTION_NAME, embeddings=embedding_model)
    print("âœ… DB ì´ˆê¸°í™” ì™„ë£Œ.")

    # 3. JSON íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    json_files = sorted(glob(os.path.join(DATA_ROOT, "**/*.json"), recursive=True))
    if not json_files:
        print(f"âŒ '{DATA_ROOT}' ê²½ë¡œì—ì„œ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    print(f"\nğŸ“Š ì´ {len(json_files)}ê°œì˜ JSON íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

    # 4. íŒŒì¼ë³„ë¡œ ë°ì´í„° ì²˜ë¦¬ ë° DBì— ì¶”ê°€
    total_docs_processed = 0
    for file_path in tqdm(json_files, desc="ì „ì²´ íŒŒì¼ ì²˜ë¦¬ ì¤‘"):
        try:
            print(f"\nğŸ“„ '{os.path.basename(file_path)}' íŒŒì¼ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì‹œì‘...")
            
            doc_generator = stream_documents_from_json(file_path)
            
            batch_for_upload = []
            # íŒŒì¼ ë‚´ì˜ ë¬¸ì„œë“¤ì„ ì‘ì€ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬í•©ë‹ˆë‹¤.
            with tqdm(desc=f"  -> '{os.path.basename(file_path)}' ì—…ë¡œë“œ ì¤‘", unit=" docs") as pbar:
                for doc in doc_generator:
                    batch_for_upload.append(doc)
                    if len(batch_for_upload) >= DOCUMENT_UPLOAD_BATCH_SIZE:
                        vectorstore.add_documents(batch_for_upload)
                        pbar.update(len(batch_for_upload))
                        total_docs_processed += len(batch_for_upload)
                        batch_for_upload = [] # ë°°ì¹˜ ì´ˆê¸°í™”
                
                # ë§ˆì§€ë§‰ì— ë‚¨ì€ ë¬¸ì„œë“¤ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
                if batch_for_upload:
                    vectorstore.add_documents(batch_for_upload)
                    pbar.update(len(batch_for_upload))
                    total_docs_processed += len(batch_for_upload)

            aggressive_memory_cleanup()
            
        except Exception as e:
            print(f"\nâš ï¸ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {os.path.basename(file_path)} - {e}")
            continue
            
    end_time = time.time()
    
    print("\n" + "="*60)
    print("ğŸ‰ ëª¨ë“  ë°ì´í„° ì²˜ë¦¬ ë° ì—…ë¡œë“œê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("="*60)
    
    final_info = client.get_collection(collection_name=COLLECTION_NAME)
    print(f"ğŸ“„ ìµœì¢… ë¬¸ì„œ(ì²­í¬) ìˆ˜: {total_docs_processed:,}ê°œ")
    print(f"ğŸ’¾ DBì— ì €ì¥ëœ ìµœì¢… í¬ì¸íŠ¸ ìˆ˜: {final_info.points_count:,}ê°œ")
    print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {(end_time - start_time) / 60:.2f}ë¶„")

if __name__ == "__main__":
    main()
