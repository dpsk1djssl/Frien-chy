# -*- coding: utf-8 -*-
# main_inference_final.py
# LangGraph + PostgreSQL Memory ê°œì„ 

import os
import uuid
from typing import List, Dict, Any, TypedDict, Optional, AsyncGenerator

import torch
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
import uvicorn
import json
import langchain
from contextlib import asynccontextmanager

from fastapi.responses import StreamingResponse
from langgraph.graph import StateGraph, END
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.chat_history import BaseChatMessageHistory
from langchain.cache import SQLiteCache

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.vectorstores import Qdrant as LCQdrant
from qdrant_client import QdrantClient

# PostgreSQL ê´€ë ¨ import
import psycopg
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage

# Qdrant ê´€ë ¨ import
from qdrant_client.models import Filter, FieldCondition, MatchValue, PayloadSchemaType

# --------------------
# Config
# --------------------
CFG_NAME = "LangGraph_With_PostgreSQL_v4.2_Improved"
EMBED_MODEL = "nlpai-lab/KURE-v1"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

K_RETRIEVE = 15  # ê²€ìƒ‰ëŸ‰ ì¦ê°€
FINAL_TOPK = 7   # ìµœì¢… ì„ íƒëŸ‰ ì¦ê°€
MIN_SCORE = float(os.getenv("QDRANT_MIN_SCORE", "0.45"))  # ì„ê³„ê°’ ì¡°ì •

# --------------------
# Builders
# --------------------
def build_embeddings():
    return HuggingFaceEmbeddings(
        model_name=EMBED_MODEL,
        model_kwargs={"device": DEVICE},
        encode_kwargs={"normalize_embeddings": True},
    )

def build_qdrant_vectorstore(embeddings):
    client = QdrantClient(
        host=os.getenv("QDRANT_HOST"),
        grpc_port=int(os.getenv("QDRANT_GRPC_PORT", "6334")),
        api_key=os.getenv("QDRANT_API_KEY"),
        timeout=60.0,
        prefer_grpc=True,
        https=False
    )
    collection = os.getenv("QDRANT_COLLECTION", "franchise-db-1")
    vs = LCQdrant(
        client=client,
        collection_name=collection,
        embeddings=embeddings,
        content_payload_key="page_content",
        metadata_payload_key="metadata",
    )
    return vs.as_retriever(search_kwargs={"k": K_RETRIEVE}), collection

def build_llm():
    return ChatGoogleGenerativeAI(
        model="gemini-2.5-pro",
        google_api_key=os.getenv("GEMINI_API_KEY"),
        temperature=0.1,
    )

def build_llm_light():
    return ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        google_api_key=os.getenv("GEMINI_API_KEY"),
        temperature=0.1,
    )

def build_llm_report():
    return ChatGoogleGenerativeAI(
        model="gemini-2.5-pro",
        google_api_key=os.getenv("GEMINI_API_KEY"),
        temperature=0.2,
    )

# --------------------
# Custom PostgreSQL Chat Message History (ê°œì„ ë¨)
# --------------------
class CustomPostgresChatMessageHistory(BaseChatMessageHistory):
    """PostgreSQL ê¸°ë°˜ì˜ ì»¤ìŠ¤í…€ ì±„íŒ… ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ (ê°œì„ ë¨)"""
    
    def __init__(self, session_id: str, connection_string: str, table_name: str = "message_store"):
        self.session_id = session_id
        self.connection_string = connection_string
        self.table_name = table_name
        self._create_table_if_not_exists()
        self._messages_cache = None  # ìºì‹± ì¶”ê°€

    def _create_table_if_not_exists(self):
        """ë©”ì‹œì§€ ì €ì¥ í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±"""
        with psycopg.connect(self.connection_string) as conn:
            with conn.cursor() as cur:
                cur.execute(f"""
                    CREATE TABLE IF NOT EXISTS {self.table_name} (
                        id SERIAL PRIMARY KEY,
                        session_id TEXT NOT NULL,
                        message_type TEXT NOT NULL,
                        content TEXT NOT NULL,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    );
                """)
                cur.execute(f"""
                    CREATE INDEX IF NOT EXISTS idx_{self.table_name}_session_id 
                    ON {self.table_name} (session_id);
                """)
                conn.commit()

    def _refresh_cache(self):
        """ë©”ì‹œì§€ ìºì‹œ ìƒˆë¡œê³ ì¹¨"""
        messages = []
        try:
            with psycopg.connect(self.connection_string) as conn:
                with conn.cursor() as cur:
                    cur.execute(
                        f"SELECT message_type, content FROM {self.table_name} "
                        f"WHERE session_id = %s ORDER BY created_at ASC",
                        (self.session_id,)
                    )
                    for message_type, content in cur.fetchall():
                        if message_type == "human":
                            messages.append(HumanMessage(content=content))
                        elif message_type == "ai":
                            messages.append(AIMessage(content=content))
        except Exception as e:
            print(f"ë©”ì‹œì§€ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        self._messages_cache = messages

    @property
    def messages(self) -> List[BaseMessage]:
        """ì„¸ì…˜ì˜ ëª¨ë“  ë©”ì‹œì§€ ì¡°íšŒ (ìºì‹œ ì‚¬ìš©)"""
        if self._messages_cache is None:
            self._refresh_cache()
        return self._messages_cache or []

    def add_message(self, message: BaseMessage) -> None:
        """ìƒˆ ë©”ì‹œì§€ ì¶”ê°€"""
        try:
            message_type = "human" if isinstance(message, HumanMessage) else "ai"
            with psycopg.connect(self.connection_string) as conn:
                with conn.cursor() as cur:
                    cur.execute(
                        f"INSERT INTO {self.table_name} (session_id, message_type, content) "
                        f"VALUES (%s, %s, %s)",
                        (self.session_id, message_type, message.content)
                    )
                    conn.commit()
            # ìºì‹œ ì—…ë°ì´íŠ¸
            if self._messages_cache is not None:
                self._messages_cache.append(message)
        except Exception as e:
            print(f"ë©”ì‹œì§€ ì¶”ê°€ ì˜¤ë¥˜: {e}")

    def clear(self) -> None:
        """ì„¸ì…˜ì˜ ëª¨ë“  ë©”ì‹œì§€ ì‚­ì œ"""
        try:
            with psycopg.connect(self.connection_string) as conn:
                with conn.cursor() as cur:
                    cur.execute(
                        f"DELETE FROM {self.table_name} WHERE session_id = %s",
                        (self.session_id,)
                    )
                    conn.commit()
            self._messages_cache = []
        except Exception as e:
            print(f"ë©”ì‹œì§€ ì‚­ì œ ì˜¤ë¥˜: {e}")

# --------------------
# Qdrant Payload Index ì„¤ì •
# --------------------
def setup_payload_indexes(client: QdrantClient, collection_name: str):
    """
    ìì£¼ ì‚¬ìš©í•˜ëŠ” ë©”íƒ€ë°ì´í„° í•„ë“œì— ëŒ€í•œ payload index ìƒì„±
    """
    try:
        # ë¸Œëœë“œëª… ì¸ë±ìŠ¤ (ì •í™•íˆ ë§¤ì¹­ë˜ëŠ” í•„í„°ë§ì— ì‚¬ìš©)
        client.create_payload_index(
            collection_name=collection_name,
            field_name="metadata.brand_name",
            field_schema=PayloadSchemaType.KEYWORD
        )
        print("brand_name ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
        
        # ì—…ì¢…(ì¤‘ë¶„ë¥˜) ì¸ë±ìŠ¤ (ì¹´í…Œê³ ë¦¬ ë§¤ì¹­ì— ì‚¬ìš©)
        client.create_payload_index(
            collection_name=collection_name,
            field_name="metadata.industry_medium",
            field_schema=PayloadSchemaType.KEYWORD
        )
        print("industry_medium ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
        
        # ì„¹ì…˜ëª… ì¸ë±ìŠ¤ (ì„¹ì…˜ë³„ ì •ë¦¬ì— ì‚¬ìš©)
        client.create_payload_index(
            collection_name=collection_name,
            field_name="metadata.section_name",
            field_schema=PayloadSchemaType.KEYWORD
        )
        print("section_name ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
        
        # ì—…ì¢…(ëŒ€ë¶„ë¥˜) ì¸ë±ìŠ¤ (ì¶”ê°€ì ì¸ ì¹´í…Œê³ ë¦¬ í•„í„°ë§ì— ì‚¬ìš© ê°€ëŠ¥)
        client.create_payload_index(
            collection_name=collection_name,
            field_name="metadata.industry_large",
            field_schema=PayloadSchemaType.KEYWORD
        )
        print("industry_large ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
        
        # ì—°ë„ ì¸ë±ìŠ¤ (ë°ì´í„° ê¸°ì¤€ì—°ë„ í•„í„°ë§ì— ì‚¬ìš© ê°€ëŠ¥)
        client.create_payload_index(
            collection_name=collection_name,
            field_name="metadata.year",
            field_schema=PayloadSchemaType.INTEGER
        )
        print("year ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
        
    except Exception as e:
        # ì´ë¯¸ ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ê±°ë‚˜ ê¸°íƒ€ ì˜¤ë¥˜ ì‹œ ê³„ì† ì§„í–‰
        print(f"âš ï¸ Payload ì¸ë±ìŠ¤ ì„¤ì • ì¤‘ ì¼ë¶€ ì˜¤ë¥˜ ë°œìƒ (ë¬´ì‹œí•˜ê³  ê³„ì†): {e}")

# --------------------
# ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œê¸° (ìƒˆë¡œ ì¶”ê°€)
# --------------------
class ContextExtractor:
    """ëŒ€í™”ì—ì„œ ë¸Œëœë“œ/ì—…ì¢… ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    
    @staticmethod
    def extract_brand_context(messages: List[BaseMessage]) -> Dict[str, Any]:
        """ìµœê·¼ ëŒ€í™”ì—ì„œ ë¸Œëœë“œ/ì—…ì¢… ì •ë³´ ì¶”ì¶œ"""
        context = {
            "mentioned_brands": set(),
            "mentioned_categories": set(),
            "last_brand": None,
            "last_category": None
        }
        
        # ìµœê·¼ 5ê°œ ë©”ì‹œì§€ë§Œ í™•ì¸ (ë„ˆë¬´ ì˜¤ë˜ëœ ê²ƒì€ ì œì™¸)
        recent_messages = messages[-5:] if len(messages) > 5 else messages
        
        brand_keywords = ["ì¹˜í‚¨", "í”¼ì", "ë²„ê±°", "ì¹´í˜", "ë„¤ë„¤", "êµì´Œ", "êµ½ë„¤", "BBQ"]
        category_keywords = ["ì™¸ì‹", "ì¹˜í‚¨", "í”¼ì", "í–„ë²„ê±°", "ì¹´í˜", "ë² ì´ì»¤ë¦¬"]
        
        for msg in reversed(recent_messages):  # ìµœì‹ ë¶€í„° í™•ì¸
            content = msg.content.lower()
            
            # ë¸Œëœë“œ í‚¤ì›Œë“œ ì°¾ê¸°
            for brand in brand_keywords:
                if brand.lower() in content:
                    context["mentioned_brands"].add(brand)
                    if not context["last_brand"]:
                        context["last_brand"] = brand
            
            # ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ì°¾ê¸°
            for category in category_keywords:
                if category.lower() in content:
                    context["mentioned_categories"].add(category)
                    if not context["last_category"]:
                        context["last_category"] = category
        
        return context



# --------------------
# ê°œì„ ëœ ë¬¸ì„œ í•„í„°ë§
# --------------------
def smart_document_filter(docs: List[Document], context: Dict[str, Any], 
                         question: str) -> List[Document]:
    """ì»¨í…ìŠ¤íŠ¸ë¥¼ ê³ ë ¤í•œ ìŠ¤ë§ˆíŠ¸ ë¬¸ì„œ í•„í„°ë§"""
    
    if not docs:
        return docs
    
    # 1. ì ìˆ˜ ê¸°ë°˜ ê¸°ë³¸ í•„í„°ë§
    scored_docs = [(d, _get_qdrant_score(d)) for d in docs]
    scored_docs.sort(key=lambda x: x[1], reverse=True)
    
    # 2. ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì ìš©
    weighted_docs = []
    question_lower = question.lower()
    
    for doc, score in scored_docs:
        content = doc.page_content.lower()
        metadata = doc.metadata or {}
        
        # ê¸°ë³¸ ì ìˆ˜
        final_score = score
        
        # ë¸Œëœë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤
        if context.get("last_brand"):
            brand = context["last_brand"].lower()
            if brand in content or brand in str(metadata.get("brand_name", "")).lower():
                final_score += 0.15
        
        # ì¹´í…Œê³ ë¦¬ ë§¤ì¹­ ë³´ë„ˆìŠ¤
        if context.get("last_category"):
            category = context["last_category"].lower()
            if category in content or category in str(metadata.get("industry_medium", "")).lower():
                final_score += 0.1
        
        # ì§ˆë¬¸ í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤
        question_keywords = ["ì°½ì—…ë¹„ìš©", "ê°€ê²©", "ìœ ì˜í• ì ", "ì¡°ê±´", "ì œí•œ", "ë¶€ë‹´"]
        for keyword in question_keywords:
            if keyword in question_lower and keyword in content:
                final_score += 0.05
        
        weighted_docs.append((doc, final_score))
    
    # 3. ìµœì¢… ì ìˆ˜ë¡œ ì¬ì •ë ¬ ë° í•„í„°ë§
    weighted_docs.sort(key=lambda x: x[1], reverse=True)
    
    # ì„ê³„ê°’ ì ìš© (ë™ì  ì¡°ì •)
    min_threshold = MIN_SCORE
    if context.get("last_brand") or context.get("last_category"):
        min_threshold -= 0.05  # ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì„ê³„ê°’ ì™„í™”
    
    filtered_docs = [doc for doc, score in weighted_docs if score >= min_threshold]
    
    # ìµœì†Œ 3ê°œëŠ” ë³´ì¥
    if len(filtered_docs) < 3 and len(weighted_docs) >= 3:
        filtered_docs = [doc for doc, _ in weighted_docs[:3]]
    
    return filtered_docs[:FINAL_TOPK]

# --------------------
# Utilities (ê°œì„ ë¨)
# --------------------
def concat_context(docs: List[Document], max_chars: int = 5000, sep: str = "\n---\n"):
    """ë¬¸ì„œë“¤ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ê²°í•© (ê¸¸ì´ ì œí•œ ì¦ê°€)"""
    out, n = [], 0
    for d in docs:
        t = (d.page_content or "").strip()
        if n + len(t) > max_chars:
            t = t[: max(0, max_chars - n)]
        out.append(t)
        n += len(t)
        if n >= max_chars:
            break
    return sep.join(out)

def serialize_doc(d: Document) -> dict:
    return {"page_content": d.page_content or "", "metadata": d.metadata or {}}

def _get_qdrant_score(d: Document) -> float:
    md = d.metadata or {}
    for k in ("_qdrant_score", "score", "similarity"):
        if k in md:
            try: 
                return float(md[k])
            except: 
                pass
    return float(getattr(d, "score", 0.0))

# --------------------
# ê°œì„ ëœ Prompts
# --------------------
ANSWER_PROMPT = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ í”„ëœì°¨ì´ì¦ˆ ì°½ì—…ì˜ ë“ ë“ í•œ ë™ë°˜ì, 'í”„ëœì¹˜'ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì•„ë˜ # ì°¾ì•„ì˜¨ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì‚¬ìš©ìì˜ ê¶ê¸ˆì¦ì„ ì‹œì›í•˜ê²Œ í•´ê²°í•´ì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤.

ì˜ˆë¹„ ì°½ì—…ìì™€ ê¸°ì¡´ ì ì£¼ë‹˜ ëª¨ë‘ê°€ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡, í•µì‹¬ë§Œ ì™ì™ ë½‘ì•„ ëª…í™•í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.

**ë‹µë³€ ì‘ì„± ê°€ì´ë“œ:**
- **ì •í™•í•œ ì •ë³´ë§Œ!:** # ì°¾ì•„ì˜¨ ì •ë³´ì— ìˆëŠ” ë‚´ìš©ë§Œì„ ê·¼ê±°ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. ì •ë³´ì— ì—†ëŠ” ë‚´ìš©ì€ 'ì•„ì§ í™•ì¸ë˜ì§€ ì•Šì€ ì •ë³´'ë¼ê³  ì†”ì§í•˜ê²Œ ë§í•´ì£¼ì„¸ìš”.
- **ë§ì¶¤í˜• ë‹µë³€!:**
    - ì •ë³´ê°€ ì¶©ë¶„í•˜ë‹¤ë©´, ì§ˆë¬¸ ì „ì²´ì— ëŒ€í•´ ëª…ì¾Œí•œ í•´ê²°ì±…ì„ ì œì‹œí•´ì£¼ì„¸ìš”.
    - ì •ë³´ê°€ ë¶€ì¡±í•˜ë‹¤ë©´, ì•„ëŠ” ë¶€ë¶„ê¹Œì§€ë§Œì´ë¼ë„ "ìš°ì„  ~ì— ëŒ€í•´ ë¨¼ì € ì„¤ëª…í•´ ë“œë¦´ê²Œìš”."ë¼ë©° ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
    - ì •ë³´ê°€ ì „í˜€ ê´€ë ¨ ì—†ë‹¤ë©´, "ì£„ì†¡í•˜ì§€ë§Œ, ë¬¸ì˜í•˜ì‹  ë‚´ìš©ê³¼ ê´€ë ¨ëœ ì •ë³´ëŠ” ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."ë¼ê³  ì •ì¤‘í•˜ê²Œ ë§ì”€í•´ì£¼ì„¸ìš”.
    - ì°¾ì•„ì˜¨ ì •ë³´ì˜ ê¸°ì¤€ì—°ë„ ëª…ì‹œí•´ì£¼ì„¸ìš”.
    - ë³„ì²¨ ìë£Œê°€ ë§ë¡œë§Œ ìˆê³  ì‹¤ì œë¡œ ì—†ë‹¤ë©´ ë¬´ì‹œí•´ì£¼ì„¸ìš”.
- **ì¹œì ˆí•œ ì¡°ì–¸!:** ë‹¨ìˆœíˆ ì •ë³´ë¥¼ ë‚˜ì—´í•˜ì§€ ë§ê³ , ì „ë¬¸ê°€ë¡œì„œ "ì´ëŸ° ì ì„ íŠ¹íˆ ìœ ì˜í•˜ì‹œë©´ ì¢‹ìŠµë‹ˆë‹¤." ì™€ ê°™ì´ ì‹¤ì§ˆì ì¸ ì¡°ì–¸ì„ ë§ë¶™ì—¬ì£¼ì„¸ìš”.

# ì§ˆë¬¸: {question}

# ì°¾ì•„ì˜¨ ì •ë³´:
{context}

# í”„ëœì¹˜ì˜ ë‹µë³€:
""".strip())

CONTEXTUALIZE_QUESTION_PROMPT = ChatPromptTemplate.from_messages(
    [
        ("system", """ì£¼ì–´ì§„ ëŒ€í™” ê¸°ë¡ê³¼ ìµœì‹  ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ì™„ì „í•œ ë…ë¦½í˜• ì§ˆë¬¸ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ì„¸ìš”.

**ì¬êµ¬ì„± ê·œì¹™:**
1. ëŒ€í™”ì—ì„œ ì–¸ê¸‰ëœ íŠ¹ì • ë¸Œëœë“œë‚˜ ì—…ì¢…ì„ ì§ˆë¬¸ì— í¬í•¨
2. "ê·¸ê²ƒ", "ê·¸ê±°", "ì´ê²ƒ" ê°™ì€ ëŒ€ëª…ì‚¬ë¥¼ êµ¬ì²´ì ì¸ ëª…ì‚¬ë¡œ êµì²´
3. ì´ì „ ë§¥ë½ì˜ í•µì‹¬ ì •ë³´ë¥¼ ì§ˆë¬¸ì— í†µí•©
4. ì§ˆë¬¸ ì™¸ì— ë‹¤ë¥¸ ì„¤ëª…ì€ ì¶”ê°€í•˜ì§€ ì•ŠìŒ

ì˜ˆì‹œ:
- ì´ì „ ëŒ€í™”: "ë„¤ë„¤ì¹˜í‚¨ì— ëŒ€í•´ ì•Œê³  ì‹¶ì–´ìš”"
- í˜„ì¬ ì§ˆë¬¸: "ì°½ì—…ë¹„ìš©ì€?"
- ì¬êµ¬ì„±ëœ ì§ˆë¬¸: "ë„¤ë„¤ì¹˜í‚¨ ì°½ì—…ë¹„ìš©ì€?"
"""),
        MessagesPlaceholder("chat_history"),
        ("user", "{question}"),
    ]
)

# --------------------
# LangGraph State & Definition
# --------------------
class GraphState(TypedDict):
    question: str
    answer: str
    docs: List[Document]
    used_docs: List[Dict]
    chat_history: List[BaseMessage]

# --------------------
# ê°œì„ ëœ Memory-Aware RAG Chain
# --------------------
class MemoryAwareRAGChain:
    def __init__(self, retriever, llm_light, llm, get_session_history):
        self.retriever = retriever
        self.llm = llm
        self.get_session_history = get_session_history
        self.contextualize_q_chain = CONTEXTUALIZE_QUESTION_PROMPT | llm_light | StrOutputParser()
        self.context_extractor = ContextExtractor()
        # self.synonym_normalizer = SynonymNormalizer()  # ğŸš€ ì„±ëŠ¥ ìµœì í™”: ì™„ì „ ì œê±°

    def _get_recent_user_questions(self, chat_history: List[BaseMessage], limit: int = 5) -> List[BaseMessage]:
        """ìµœê·¼ ì‚¬ìš©ì ì§ˆë¬¸ë§Œ ì¶”ì¶œ (AI ë‹µë³€ ì œì™¸)"""
        user_questions = [msg for msg in chat_history if isinstance(msg, HumanMessage)]
        return user_questions[-limit:] if len(user_questions) > limit else user_questions

    def invoke(self, input_data: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ê¸°ëŠ¥ì„ í¬í•¨í•œ ê°œì„ ëœ RAG ì²˜ë¦¬"""
        session_id = config.get("configurable", {}).get("session_id")
        if not session_id:
            raise ValueError("session_idê°€ configì— ì—†ìŠµë‹ˆë‹¤.")
        
        # ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
        history = self.get_session_history(session_id)
        chat_history = history.messages
        
        question = input_data["question"]
        print(f"=== RAG ì²˜ë¦¬ ì‹œì‘ ===")
        print(f"ì›ë³¸ ì§ˆë¬¸: {question}")
        print(f"ê¸°ì¡´ ëŒ€í™” ê¸°ë¡: {len(chat_history)}ê°œ ë©”ì‹œì§€")
        
        # ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
        context_info = self.context_extractor.extract_brand_context(chat_history)
        print(f"ì¶”ì¶œëœ ì»¨í…ìŠ¤íŠ¸: {context_info}")
        
        # ëŒ€í™” ê¸°ë¡ì´ ìˆìœ¼ë©´ ì§ˆë¬¸ì„ ë§¥ë½í™” (ìµœê·¼ ì‚¬ìš©ì ì§ˆë¬¸ 5ê°œë§Œ ì‚¬ìš©)
        if chat_history:
            recent_user_questions = self._get_recent_user_questions(chat_history, 5)
            contextualized_question = self.contextualize_q_chain.invoke(
                {"question": question, "chat_history": recent_user_questions}
            )
            print(f"ë§¥ë½í™”ëœ ì§ˆë¬¸: {contextualized_question}")
            print(f"ì‚¬ìš©ëœ ì‚¬ìš©ì ì§ˆë¬¸ ìˆ˜: {len(recent_user_questions)}")
        else:
            contextualized_question = question
            print("ëŒ€í™” ê¸°ë¡ ì—†ìŒ, ì›ë³¸ ì§ˆë¬¸ ì‚¬ìš©")
        
        # ìœ ì‚¬ì–´ ê¸°ë°˜ ì¬ì‘ì„±(í‘œì¤€í™”/í™•ì¥) - ğŸš€ ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•´ ì™„ì „ ë¹„í™œì„±í™”
        # rewritten_question = self.synonym_normalizer.rewrite(contextualized_question)
        rewritten_question = contextualized_question  # ì„±ëŠ¥ ìµœì í™”: ìœ ì‚¬ì–´ ì¬ì‘ì„± ìƒëµ
        print(f"âš¡ ì„±ëŠ¥ ìµœì í™”: ìœ ì‚¬ì–´ ì¬ì‘ì„± ì™„ì „ ìƒëµ")
        print(f"ìµœì¢… ê²€ìƒ‰ ì§ˆë¬¸: {rewritten_question}")

        # ë¬¸ì„œ ê²€ìƒ‰ (ì¬ì‘ì„±ëœ ì§ˆë¬¸ìœ¼ë¡œ)
        raw_docs = self.retriever.invoke(rewritten_question)
        print(f"ê²€ìƒ‰ëœ ì›ë³¸ ë¬¸ì„œ ìˆ˜: {len(raw_docs)}")
        
        # ìŠ¤ë§ˆíŠ¸ í•„í„°ë§ ì ìš©
        final_docs = smart_document_filter(raw_docs, context_info, question)
        print(f"í•„í„°ë§ í›„ ìµœì¢… ë¬¸ì„œ ìˆ˜: {len(final_docs)}")
        
        # ì‚¬ìš©ëœ ë¬¸ì„œì˜ ë¸Œëœë“œ ì •ë³´ ì¶œë ¥ (ë” ìƒì„¸í•˜ê²Œ)
        for i, doc in enumerate(final_docs):
            brand = doc.metadata.get("brand_name", "N/A")
            category = doc.metadata.get("industry_medium", "N/A")
            section = doc.metadata.get("section_name", "N/A")[:50]  # 50ìê¹Œì§€ë§Œ
            score = _get_qdrant_score(doc)
            print(f"ë¬¸ì„œ {i+1}: {brand} ({category}) | {section}... | ì ìˆ˜: {score:.3f}")
        
        # ë‹µë³€ ìƒì„± (ì›ë³¸ ì§ˆë¬¸ìœ¼ë¡œ)
        if not final_docs:
            answer = "ìš”ì²­í•˜ì‹  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        else:
            ctx = concat_context(final_docs)
            rag_chain = (
                RunnablePassthrough.assign(context=lambda x: ctx)
                | ANSWER_PROMPT
                | self.llm
                | StrOutputParser()
            )
            answer = rag_chain.invoke({"question": question})
        
        print(f"ìƒì„±ëœ ë‹µë³€: {answer}")
        
        # ëŒ€í™” ê¸°ë¡ì— ì €ì¥
        history.add_message(HumanMessage(content=question))
        history.add_message(AIMessage(content=answer))
        
        return {
            "question": question,
            "answer": answer,
            "used_docs": [serialize_doc(d) for d in final_docs]
        }
    
    # NEW: ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ë¹„ë™ê¸° ì œë„ˆë ˆì´í„° ë©”ì†Œë“œ ì¶”ê°€
    async def astream_invoke(self, input_data: Dict[str, Any], config: Dict[str, Any]) -> AsyncGenerator[str, None]:
        """ë©”ëª¨ë¦¬ ê¸°ëŠ¥ì„ í¬í•¨í•œ ê°œì„ ëœ RAG ì²˜ë¦¬ (ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë°)"""
        session_id = config.get("configurable", {}).get("session_id")
        if not session_id:
            raise ValueError("session_idê°€ configì— ì—†ìŠµë‹ˆë‹¤.")

        question = input_data["question"]
        
        print("=== RAG ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì‹œì‘ ===")
        print(f"ì›ë³¸ ì§ˆë¬¸: {question}")

        # ğŸš€ ì¦‰ì‹œ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: ì²˜ë¦¬ ì‹œì‘ ì•Œë¦¼
        yield f"event: status\ndata: {json.dumps({'status': 'processing', 'step': 'started'}, ensure_ascii=False)}\n\n"

        # 1. ëŒ€í™” ê¸°ë¡ ì¡°íšŒ
        history = self.get_session_history(session_id)
        chat_history = history.messages
        yield f"event: status\ndata: {json.dumps({'status': 'processing', 'step': 'context_loading'}, ensure_ascii=False)}\n\n"

        # 2. ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
        context_info = self.context_extractor.extract_brand_context(chat_history)
        yield f"event: status\ndata: {json.dumps({'status': 'processing', 'step': 'context_extracted'}, ensure_ascii=False)}\n\n"
        
        # 3. ì§ˆë¬¸ ë§¥ë½í™”
        if chat_history:
            yield f"event: status\ndata: {json.dumps({'status': 'processing', 'step': 'contextualizing_question'}, ensure_ascii=False)}\n\n"
            recent_user_questions = self._get_recent_user_questions(chat_history, 5)
            contextualized_question = self.contextualize_q_chain.invoke(
                {"question": question, "chat_history": recent_user_questions}
            )
        else:
            contextualized_question = question
        
        # ğŸš€ ì„±ëŠ¥ ìµœì í™”: ìœ ì‚¬ì–´ ì¬ì‘ì„± ì™„ì „ ìƒëµ
        rewritten_question = contextualized_question
        print(f"âš¡ ì„±ëŠ¥ ìµœì í™”: ìœ ì‚¬ì–´ ì¬ì‘ì„± ì™„ì „ ìƒëµ")
        print(f"ìµœì¢… ê²€ìƒ‰ ì§ˆë¬¸: {rewritten_question}")

        # 4. ë¬¸ì„œ ê²€ìƒ‰ ì‹œì‘ ì•Œë¦¼
        yield f"event: status\ndata: {json.dumps({'status': 'processing', 'step': 'searching_documents'}, ensure_ascii=False)}\n\n"
        
        raw_docs = self.retriever.invoke(rewritten_question)
        final_docs = smart_document_filter(raw_docs, context_info, question)
        print(f"ìµœì¢… ë¬¸ì„œ ìˆ˜: {len(final_docs)}")
        
        # 5. ê²€ìƒ‰ ì™„ë£Œ ë° ì°¸ê³  ë¬¸ì„œ ì •ë³´ ì „ì†¡
        yield f"event: status\ndata: {json.dumps({'status': 'processing', 'step': 'documents_found', 'count': len(final_docs)}, ensure_ascii=False)}\n\n"
        
        used_docs_json = json.dumps([serialize_doc(d) for d in final_docs], ensure_ascii=False)
        yield f"event: sources\ndata: {used_docs_json}\n\n"

        # 6. ë‹µë³€ ìƒì„± ì‹œì‘ ì•Œë¦¼
        yield f"event: status\ndata: {json.dumps({'status': 'processing', 'step': 'generating_answer'}, ensure_ascii=False)}\n\n"

        # 7. ë‹µë³€ ìƒì„± ìŠ¤íŠ¸ë¦¬ë°
        if not final_docs:
            answer_chunk = "ìš”ì²­í•˜ì‹  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            yield f"data: {json.dumps({'token': answer_chunk}, ensure_ascii=False)}\n\n"
            full_answer = answer_chunk
        else:
            ctx = concat_context(final_docs)
            rag_chain = (
                RunnablePassthrough.assign(context=lambda x: ctx)
                | ANSWER_PROMPT
                | self.llm
                | StrOutputParser()
            )
            
            full_answer = ""
            # .astream()ì„ ì‚¬ìš©í•˜ì—¬ ë¹„ë™ê¸°ì ìœ¼ë¡œ í† í°ì„ ë°›ì•„ì˜µë‹ˆë‹¤.
            async for chunk in rag_chain.astream({"question": question}):
                full_answer += chunk
                # ê° í† í°(chunk)ì„ SSE í˜•ì‹ìœ¼ë¡œ í´ë¼ì´ì–¸íŠ¸ì— yield
                yield f"data: {json.dumps({'token': chunk}, ensure_ascii=False)}\n\n"

        # 4. ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ í›„ ëŒ€í™” ê¸°ë¡ ì €ì¥
        history.add_message(HumanMessage(content=question))
        history.add_message(AIMessage(content=full_answer))
        print(f"ì „ì²´ ë‹µë³€ ì €ì¥ ì™„ë£Œ: {full_answer}")

        # 5. ìŠ¤íŠ¸ë¦¼ì˜ ëì„ ì•Œë¦¬ëŠ” íŠ¹ë³„ ì´ë²¤íŠ¸ ì „ì†¡
        yield "event: end\ndata: Stream ended\n\n"

# --------------------
# FastAPI App (ê¸°ì¡´ê³¼ ë™ì¼í•˜ì§€ë§Œ ê°œì„ ëœ ì²´ì¸ ì‚¬ìš©)
# --------------------
class QuestionRequest(BaseModel):
    question: str
    session_id: str = Field(..., description="ê° ì‚¬ìš©ìì˜ ëŒ€í™”ë¥¼ êµ¬ë¶„í•˜ëŠ” ê³ ìœ  ID")

class AnswerResponse(BaseModel):
    question: str
    answer: str
    used_docs: List[Dict[str, Any]]
    status: str
    cfg: str

app = FastAPI(title="í”„ëœì°¨ì´ì¦ˆ QA API (ê°œì„ ëœ ë©”ëª¨ë¦¬)", version="4.2.0")

# PostgreSQL ì—°ê²° ì •ë³´
DATABASE_URL = os.getenv("DATABASE_URL")
if not DATABASE_URL:
    raise RuntimeError("DATABASE_URL í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# PostgreSQL ì—°ê²° ì •ë³´ (ë™ì˜ì–´ DBëŠ” ì œê±°ë¨ - ì„±ëŠ¥ ìµœì í™”)
# DATABASE_URL_SYN = os.getenv("DATABASE_SYN_URL")  # ğŸš€ ì„±ëŠ¥ ìµœì í™”: ì œê±°

def get_session_history(session_id: str) -> BaseChatMessageHistory:
    """PostgreSQL ê¸°ë°˜ ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ë°˜í™˜"""
    return CustomPostgresChatMessageHistory(
        session_id=session_id,
        connection_string=DATABASE_URL,
        table_name="message_store"
    )

# --------------------
# ë¸Œëœë“œ ë¦¬í¬íŠ¸ìš© Request/Response Models
# --------------------
class BrandReportRequest(BaseModel):
    brand_name: str = Field(..., description="ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•  ë¸Œëœë“œëª…")
    session_id: Optional[str] = Field(None, description="ì„¸ì…˜ ID (ì„ íƒì‚¬í•­)")

class BrandReportResponse(BaseModel):
    brand_name: str
    report: str
    total_docs: int
    sections_covered: List[str]
    status: str
    cfg: str

# --------------------
# ë¸Œëœë“œ ë¦¬í¬íŠ¸ ìƒì„±ìš© Prompt
# --------------------
BRAND_REPORT_PROMPT = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ í”„ëœì°¨ì´ì¦ˆ ë¸Œëœë“œ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. 
ì£¼ì–´ì§„ {brand_name} ë¸Œëœë“œì˜ ëª¨ë“  ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•©ì ì¸ ë¸Œëœë“œ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

**ë¦¬í¬íŠ¸ ì‘ì„± ì§€ì¹¨:**
1. ì²´ê³„ì ì´ê³  êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ì‘ì„±
2. í•µì‹¬ ì •ë³´ë¥¼ ì„¹ì…˜ë³„ë¡œ ëª…í™•íˆ êµ¬ë¶„
3. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ì¡°ê±´ì„ í¬í•¨
4. ì¥ë‹¨ì ì„ ê°ê´€ì ìœ¼ë¡œ ë¶„ì„
5. ì˜ˆë¹„ ì°½ì—…ìì—ê²Œ ìœ ìš©í•œ ì¸ì‚¬ì´íŠ¸ ì œê³µ

**í•„ìˆ˜ í¬í•¨ ì„¹ì…˜:**
- ë¸Œëœë“œ ê°œìš”
- ì°½ì—… ë¹„ìš© ë° ì¡°ê±´
- ìš´ì˜ ì‹œìŠ¤í…œ ë° ì§€ì›
- ìˆ˜ìµì„± ë¶„ì„
- ê²½ìŸë ¥ ë° ì°¨ë³„í™” ìš”ì†Œ
- ì£¼ì˜ì‚¬í•­ ë° ì œì•½ì¡°ê±´
- ì¢…í•© í‰ê°€ ë° ì¶”ì²œ ëŒ€ìƒ

# ë¸Œëœë“œëª…: {brand_name}

# ìˆ˜ì§‘ëœ ì •ë³´:
{context}

# ì¢…í•© ë¦¬í¬íŠ¸:
""".strip())

# --------------------
# Qdrant Scrollì„ ì‚¬ìš©í•œ ë¬¸ì„œ ìˆ˜ì§‘ í•¨ìˆ˜
# --------------------
def collect_brand_documents(client: QdrantClient, collection_name: str, 
                           brand_name: str, batch_size: int = 100) -> List[Dict]:
    """
    Qdrant scrollì„ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ë¸Œëœë“œì˜ ëª¨ë“  ë¬¸ì„œë¥¼ ìˆ˜ì§‘
    """
    all_documents = []
    offset = None
    
    # ë¸Œëœë“œëª… í•„í„° ìƒì„±
    filter_condition = Filter(
        must=[
            FieldCondition(
                key="metadata.brand_name",
                match=MatchValue(value=brand_name)
            )
        ]
    )
    
    while True:
        # Scroll ìš”ì²­
        result = client.scroll(
            collection_name=collection_name,
            scroll_filter=filter_condition,
            limit=batch_size,
            offset=offset,
            with_payload=True,
            with_vectors=False  # ë²¡í„°ëŠ” í•„ìš”ì—†ìŒ
        )
        
        points, next_offset = result
        
        if not points:
            break
            
        # ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ
        for point in points:
            payload = point.payload
            doc_info = {
                "content": payload.get("page_content", ""),
                "metadata": payload.get("metadata", {}),
                "section": payload.get("metadata", {}).get("section_name", "ê¸°íƒ€")
            }
            all_documents.append(doc_info)
        
        offset = next_offset
        if offset is None:
            break
    
    return all_documents

# --------------------
# ë¬¸ì„œ ì •ë¦¬ ë° êµ¬ì¡°í™” í•¨ìˆ˜
# --------------------
def organize_documents_by_section(documents: List[Dict]) -> Dict[str, List[str]]:
    """
    ë¬¸ì„œë¥¼ ì„¹ì…˜ë³„ë¡œ ì •ë¦¬
    """
    organized = {}
    
    for doc in documents:
        section = doc["section"]
        content = doc["content"]
        
        if section not in organized:
            organized[section] = []
        
        # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ê°„ë‹¨í•œ ì²´í¬
        if content not in organized[section]:
            organized[section].append(content)
    
    return organized


@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    print("Setting up SQLite cache for LangChain...")
    langchain.llm_cache = SQLiteCache(database_path="langchain.db")
    print("Cache setup complete.")
    
    # DB ì—°ê²° í…ŒìŠ¤íŠ¸
    try:
        test_history = get_session_history("test_connection")
        print("PostgreSQL ì—°ê²° ì„±ê³µ!")
    except Exception as e:
        print(f"PostgreSQL ì—°ê²° ì‹¤íŒ¨: {e}")
        raise
    
    embeddings = build_embeddings()
    retriever, collection = build_qdrant_vectorstore(embeddings)
    llm = build_llm()
    llm_light = build_llm_light()
    llm_report = build_llm_report()
    
    # Qdrant Payload Index ì„¤ì •
    print("Qdrant Payload Index ì„¤ì • ì¤‘...")
    try:
        client = QdrantClient(
            host=os.getenv("QDRANT_HOST"),
            grpc_port=int(os.getenv("QDRANT_GRPC_PORT", "6334")),
            api_key=os.getenv("QDRANT_API_KEY"),
            timeout=60.0,
            prefer_grpc=True,
            https=False
        )
        setup_payload_indexes(client, collection)
        print("Payload Index ì„¤ì • ì™„ë£Œ!")
    except Exception as e:
        print(f"âš ï¸ Payload Index ì„¤ì • ì‹¤íŒ¨ (ì•±ì€ ê³„ì† ì‹¤í–‰ë©ë‹ˆë‹¤): {e}")
    
    # ê°œì„ ëœ ë©”ëª¨ë¦¬ ì¸ì‹ RAG ì²´ì¸ ìƒì„±
    app.state.memory_rag_chain = MemoryAwareRAGChain(retriever, llm_light, llm, get_session_history)
    app.state.collection = collection
    app.state.llm_report = llm_report
    app.state.qdrant_client = client  # âœ¨ Qdrant í´ë¼ì´ì–¸íŠ¸ë¥¼ app.stateì— ì €ì¥
    
    yield
    
    # Shutdown (í•„ìš”ì‹œ ì •ë¦¬ ì‘ì—…)
    print("Application shutdown...")


app = FastAPI(title="í”„ëœì°¨ì´ì¦ˆ QA API (ê°œì„ ëœ ë©”ëª¨ë¦¬)", version="4.2.0", lifespan=lifespan)

# âœ¨ CHANGED: /ask ì—”ë“œí¬ì¸íŠ¸ë¥¼ ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë³€ê²½
@app.post("/ask") # â›”ï¸ response_model=AnswerResponse ì œê±°
async def ask(req: QuestionRequest): # âœ¨ async defë¡œ ë³€ê²½
    if not req.question.strip():
        raise HTTPException(status_code=400, detail="ì§ˆë¬¸ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
    if not req.session_id.strip():
        raise HTTPException(status_code=400, detail="session_idê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

    try:
        session_uuid = str(uuid.uuid5(uuid.NAMESPACE_DNS, req.session_id.strip()))
        config = {"configurable": {"session_id": session_uuid}}

        # StreamingResponseë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        # contentëŠ” ë¹„ë™ê¸° ì œë„ˆë ˆì´í„° í•¨ìˆ˜ í˜¸ì¶œ ê·¸ ìì²´ì…ë‹ˆë‹¤.
        return StreamingResponse(
            app.state.memory_rag_chain.astream_invoke(
                {"question": req.question.strip()},
                config=config
            ),
            media_type="text/event-stream" # âœ¨ SSEë¥¼ ìœ„í•œ media type
        )
    except Exception as e:
        import traceback
        traceback.print_exc()
        # ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜ëŠ” ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ 500 ì—ëŸ¬ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤.
        raise HTTPException(status_code=500, detail=str(e))

# ê±´ê°• ì²´í¬ ë° DB ìƒíƒœ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸
@app.get("/health")
def health_check():
    try:
        with psycopg.connect(DATABASE_URL) as conn:
            with conn.cursor() as cur:
                cur.execute("SELECT 1")
        db_status = "connected"
    except Exception as e:
        db_status = f"error: {str(e)}"
    
    return {
        "status": "healthy", 
        "config": CFG_NAME,
        "database": db_status
    }

# ì„¸ì…˜ ê¸°ë¡ ì¡°íšŒ ì—”ë“œí¬ì¸íŠ¸ (ë””ë²„ê¹…ìš©)
@app.get("/debug/history/{session_id}")
def get_debug_history(session_id: str):
    try:
        session_uuid = str(uuid.uuid5(uuid.NAMESPACE_DNS, session_id.strip()))
        history = get_session_history(session_uuid)
        messages = history.messages
        context_info = ContextExtractor.extract_brand_context(messages)
        
        return {
            "session_id": session_id,
            "session_uuid": session_uuid,
            "message_count": len(messages),
            "context_info": {
                "mentioned_brands": list(context_info["mentioned_brands"]),
                "mentioned_categories": list(context_info["mentioned_categories"]),
                "last_brand": context_info["last_brand"],
                "last_category": context_info["last_category"]
            },
            "messages": [{"type": type(msg).__name__, "content": msg.content} for msg in messages[-10:]]  # ìµœê·¼ 10ê°œë§Œ
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ì„¸ì…˜ ê¸°ë¡ ì‚­ì œ ì—”ë“œí¬ì¸íŠ¸ (ë””ë²„ê¹…ìš©)
@app.delete("/debug/history/{session_id}")
def clear_debug_history(session_id: str):
    try:
        session_uuid = str(uuid.uuid5(uuid.NAMESPACE_DNS, session_id.strip()))
        history = get_session_history(session_uuid)
        history.clear()
        return {"message": "ì„¸ì…˜ ê¸°ë¡ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.", "session_id": session_id}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# Qdrant ì¸ë±ìŠ¤ ìƒíƒœ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸ (ë””ë²„ê¹…ìš©)
@app.get("/debug/qdrant/indexes")
def check_qdrant_indexes():
    """Qdrant ì»¬ë ‰ì…˜ì˜ payload index ìƒíƒœ í™•ì¸"""
    try:
        client = app.state.qdrant_client  # âœ¨ ì¬ì‚¬ìš©ëœ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
        collection_name = app.state.collection
        collection_info = client.get_collection(collection_name)
        
        # payload schema ì •ë³´ì—ì„œ ì¸ë±ìŠ¤ í™•ì¸
        payload_schema = collection_info.config.params.vectors.get("default", {}).get("size", 0)
        
        # ì»¬ë ‰ì…˜ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        return {
            "collection_name": collection_name,
            "collection_status": collection_info.status,
            "points_count": collection_info.points_count,
            "payload_schema": collection_info.config.payload_index if hasattr(collection_info.config, 'payload_index') else "ì •ë³´ ì—†ìŒ",
            "message": "ì¸ë±ìŠ¤ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”. ìì£¼ ì‚¬ìš©í•˜ëŠ” í•„ë“œì— ì¸ë±ìŠ¤ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Qdrant ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {str(e)}")

# --------------------
# ë¸Œëœë“œ ë¦¬í¬íŠ¸ ìƒì„± ì—”ë“œí¬ì¸íŠ¸
# --------------------
@app.post("/brand-report", response_model=BrandReportResponse)
async def generate_brand_report(req: BrandReportRequest):
    """
    íŠ¹ì • ë¸Œëœë“œì˜ ëª¨ë“  ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
    """
    if not req.brand_name.strip():
        raise HTTPException(status_code=400, detail="ë¸Œëœë“œëª…ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
    
    try:
        brand_name = req.brand_name.strip()
        print(f"=== ë¸Œëœë“œ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘: {brand_name} ===")
        
        # âœ¨ app.stateì—ì„œ ì¬ì‚¬ìš©ëœ Qdrant í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
        client = app.state.qdrant_client
        collection_name = app.state.collection
        
        # 1. ë¸Œëœë“œ ê´€ë ¨ ëª¨ë“  ë¬¸ì„œ ìˆ˜ì§‘
        print(f"Qdrantì—ì„œ {brand_name} ë¬¸ì„œ ìˆ˜ì§‘ ì¤‘...")
        all_docs = collect_brand_documents(client, collection_name, brand_name)
        print(f"ìˆ˜ì§‘ëœ ë¬¸ì„œ ìˆ˜: {len(all_docs)}")
        
        if not all_docs:
            return BrandReportResponse(
                brand_name=brand_name,
                report=f"{brand_name}ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¸Œëœë“œëª…ì„ ì •í™•íˆ ì…ë ¥í–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.",
                total_docs=0,
                sections_covered=[],
                status="no_data",
                cfg=CFG_NAME
            )
        
        # 2. ë¬¸ì„œë¥¼ ì„¹ì…˜ë³„ë¡œ ì •ë¦¬
        organized_docs = organize_documents_by_section(all_docs)
        sections_covered = list(organized_docs.keys())
        print(f"ì»¤ë²„ëœ ì„¹ì…˜: {sections_covered}")
        
        # 3. ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ì„¹ì…˜ë³„ë¡œ êµ¬ì¡°í™”)
        context_parts = []
        for section, contents in organized_docs.items():
            context_parts.append(f"\n## {section}")
            # ê° ì„¹ì…˜ë‹¹ ìµœëŒ€ 5ê°œ ë¬¸ì„œë§Œ í¬í•¨ (ë„ˆë¬´ ê¸¸ì–´ì§€ëŠ” ê²ƒ ë°©ì§€)
            for content in contents[:5]:
                # ê° ë¬¸ì„œë¥¼ 500ìë¡œ ì œí•œ
                truncated = content[:500] + "..." if len(content) > 500 else content
                context_parts.append(f"- {truncated}")
        
        context = "\n".join(context_parts)
        
        # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (í† í° ì œí•œ ê³ ë ¤)
        max_context_length = 15000  # ì•½ 3750 í† í°
        if len(context) > max_context_length:
            context = context[:max_context_length] + "\n... (ì¶”ê°€ ì •ë³´ ìƒëµ)"
        
        # 4. LLMì„ ì‚¬ìš©í•˜ì—¬ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
        llm_report = app.state.llm_report
        
        report_chain = (
            BRAND_REPORT_PROMPT
            | llm_report
            | StrOutputParser()
        )
        
        report = report_chain.invoke({
            "brand_name": brand_name,
            "context": context
        })
        
        print(f"ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ. ê¸¸ì´: {len(report)}ì")
        
        # 5. ì„¸ì…˜ íˆìŠ¤í† ë¦¬ì— ì €ì¥ (ì„ íƒì‚¬í•­)
        if req.session_id:
            try:
                session_uuid = str(uuid.uuid5(uuid.NAMESPACE_DNS, req.session_id.strip()))
                history = get_session_history(session_uuid)
                history.add_message(HumanMessage(content=f"{brand_name} ë¸Œëœë“œ ë¦¬í¬íŠ¸ ìš”ì²­"))
                history.add_message(AIMessage(content=f"[ë¸Œëœë“œ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ - {len(all_docs)}ê°œ ë¬¸ì„œ ë¶„ì„]"))
            except Exception as e:
                print(f"ì„¸ì…˜ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        return BrandReportResponse(
            brand_name=brand_name,
            report=report,
            total_docs=len(all_docs),
            sections_covered=sections_covered,
            status="success",
            cfg=CFG_NAME
        )
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
